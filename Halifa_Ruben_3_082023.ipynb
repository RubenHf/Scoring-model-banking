{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b17623",
   "metadata": {},
   "source": [
    "# Table des matières\n",
    "\n",
    "### [Importation du jeu de données](#T1)\n",
    "- [Importation des librairies](#T1C1)\n",
    "- [Importation du jeu de données](#T1C2)\n",
    "- [Fonctions](#T1C3)\n",
    "\n",
    "### [Développement du modèle : LogisticRegression](#T2)\n",
    "- [Score métier](#T2C1)\n",
    "- [Recherche du meilleur seuil](#T2C2)\n",
    "- [Recherche des meilleurs hyperparamètres](#T2C3)\n",
    "- [Quelques observation sur le modèle](#T2C4)\n",
    "- [Enregistrement du modèle](#T2C5)\n",
    "\n",
    "### [Dérive du modèle](#T3)\n",
    "- [Evidently](#T3C1)\n",
    "- [Résultat](#T3C2)\n",
    "\n",
    "### [Conclusions générales](#C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c8235",
   "metadata": {},
   "source": [
    "***\n",
    "# <a name=\"T1\">Importation du jeu de données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cbcc4",
   "metadata": {},
   "source": [
    "### <a name = \"T1C1\">a. Importation des librairies</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875a0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "Blowfish has been deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version : 1.5.3\n",
      "Numpy Version : 1.23.5\n",
      "Scikit-Learn Version : 1.2.2\n",
      "Imblearn Version : 0.11.0\n",
      "mlflow Version : 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sklearn\n",
    "import imblearn\n",
    "import shap\n",
    "import random\n",
    "import mlflow\n",
    "import copy\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, IFrame\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold \n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, fbeta_score\n",
    "\n",
    "from imblearn.pipeline import Pipeline # sklearn ne prend pas en compte les méthodes de sampling\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from mlflow.sklearn import log_model, save_model, autolog\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "libraries = {\n",
    "    \"Pandas\" : pd,\n",
    "    \"Numpy\" : np,\n",
    "    \"Scikit-Learn\" : sklearn,\n",
    "    \"Imblearn\" : imblearn,\n",
    "    \"mlflow\" : mlflow,\n",
    "}\n",
    "\n",
    "# On affiche les différentes versions des librairies utilisées\n",
    "for lib_name, lib in libraries.items():\n",
    "    print(f\"{lib_name} Version : {lib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832bb43",
   "metadata": {},
   "source": [
    "### <a name = \"T1C2\">b. Importation du jeu de données</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9a5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/mohammad2012191/reduce-memory-usage-2gb-780mb\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"L'utilisation de la mémoire est de {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for i, col in enumerate(df.columns, start=1):\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Calculate the percentage of completion\n",
    "        completion_percentage = (i / df.shape[1]) * 100\n",
    "\n",
    "        # Use \\r to move the cursor to the beginning of the line and overwrite the previous output\n",
    "        print(f\"({i}/{df.shape[1]}) - {completion_percentage:.2f}% complete\", end='\\r')\n",
    "\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"L'utilisation de la mémoire est de {:.2f} MB après optimisation\".format(end_mem))\n",
    "    print(\"Réduction de {:.1f} %\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82d211d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'utilisation de la mémoire est de 1869.84 MB\n",
      "L'utilisation de la mémoire est de 515.26 MB après optimisation\n",
      "Réduction de 72.4 %\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"fichiers/concatenate_files.csv\")\n",
    "# On réduit la taille du fichier\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc10d0",
   "metadata": {},
   "source": [
    "### <a name = \"T1C3\">c. Fonctions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b6d21",
   "metadata": {},
   "source": [
    "On utilise les mêmes fonctions que dans le précédent notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4974bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprise des explications du kaggle.\n",
    "# La plupart des features sont créées en appliquant les fonctions min, max, moyenne, somme et var.\n",
    "# - La division de certaines features permet d'obtenir des taux (comme la rente et le revenu)\n",
    "# - Dans Bureau Data : créer des features spécifiques pour les crédits actifs et les crédits clôturés\n",
    "# - Dans Previous Applications : on a des features spécifiques pour les candidatures approuvées et refusées\n",
    "# - On réalise un One Hot encodage pour les featurees catégoriques\n",
    "# Tous les fichiers sont joints au fichier application avec SK_ID_CURR (sauf bureau_balance).\n",
    "\n",
    "\n",
    "# Pour limiter le nombre d'entrées, lors de test\n",
    "DEBUG = None # 10000\n",
    "\n",
    "# Permet de réaliser le One Hot encodage\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess les fichiers application_train.csv / application_test.csv \n",
    "def application_train_test(file, nan_as_category = False):\n",
    "    df = pd.read_csv(file, nrows = DEBUG)\n",
    "    print(\"Samples: {}\".format(len(df)))\n",
    "    \n",
    "    # On élimine les entrées CODE_GENDER == XNA (genre non spécifié) et qui ne concernent que peu de clients\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # On encode par 0 ou 1 quand il n'y a que 2 catégories\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Sinon on fait du one hot encoding\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # On remplace les erreurs entrées par des np.NaN \n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    \n",
    "    # Quelques features créées par feature engineering\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    \n",
    "    # On met en positif, puis on divise par 365 pour avoir le nombre d'années\n",
    "    df[\"ANNEES_AGE\"] = round((abs(df.DAYS_BIRTH) / 365.25), 1)\n",
    "    df[\"ANNEES_EMPLOYED\"] = round((abs(df.DAYS_EMPLOYED) / 365.25), 1)\n",
    "    df[\"ANNEES_LAST_PHONE_CHANGE\"] = round((abs(df.DAYS_LAST_PHONE_CHANGE) / 365.25), 2)    \n",
    "    \n",
    "    # On élimine les anciennes variables\n",
    "    df.drop([\"DAYS_BIRTH\", \"DAYS_EMPLOYED\", \"DAYS_LAST_PHONE_CHANGE\"], axis = 1, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess les fichiers bureau.csv et bureau_balance.csv\n",
    "def bureau_and_balance(file_bureau = \"fichiers/bureau.csv\", file_bb = \"fichiers/bureau_balance.csv\", nan_as_category = True):\n",
    "    bureau = pd.read_csv(file_bureau, nrows = DEBUG)\n",
    "    bb = pd.read_csv(file_bb, nrows = DEBUG)\n",
    "    \n",
    "    # On réalise un One Hot encodage\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance : On réalise une aggrégation puis on fusionne avec bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    \n",
    "    # Elimination des dataframes pour libérer de la mémoire\n",
    "    del bb, bb_agg\n",
    "    \n",
    "    \n",
    "    # On réalise différentes aggrégations sur les features numériques\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    \n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # Elimination des dataframes pour libérer de la mémoire\n",
    "    del active, active_agg\n",
    "    \n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Elimination des dataframes pour libérer de la mémoire\n",
    "    del closed, closed_agg, bureau\n",
    "    \n",
    "    return bureau_agg\n",
    "\n",
    "# Preprocess du fichier previous_applications.csv\n",
    "def previous_applications(file = \"fichiers/previous_application.csv\", nan_as_category = True):\n",
    "    prev = pd.read_csv(file, nrows = DEBUG)\n",
    "    # One hot encodage\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    \n",
    "    # On remplace les erreurs entrées par des np.NaN \n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    \n",
    "    # Feature engineering : valeur demandée / valeur reçue \n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Correction\n",
    "    prev['APP_CREDIT_PERC'] = prev['APP_CREDIT_PERC'].apply(lambda x: 0 if (x == np.nan) or (x == np.inf) else x)\n",
    "    # Features numériques\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Features catégoriques\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    \n",
    "    # On aggrége sur les anciennes applications qui ont été acceptées \n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # On aggrége sur les anciennes applications qui ont été refusées \n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # Elimination des dataframes pour libérer de la mémoire\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    \n",
    "    return prev_agg\n",
    "\n",
    "# Preprocess fichier POS_CASH_balance.csv\n",
    "def pos_cash(file = \"fichiers/POS_CASH_balance.csv\", nan_as_category = True):\n",
    "    pos = pd.read_csv(file, nrows = DEBUG)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    \n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    \n",
    "    # On compte le nombre de POS compte\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    # Elimination du dataframe pour libérer de la mémoire\n",
    "    del pos\n",
    "    \n",
    "    return pos_agg\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(file = \"fichiers/installments_payments.csv\", nan_as_category = True):\n",
    "    ins = pd.read_csv(file, nrows = DEBUG)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    \n",
    "    # Quelques features créées par feature engineering\n",
    "    # Pourcentage et différence payées sur les différents prêts (Pourcentage payé et ce qu'il reste à payer)\n",
    "    ins['AMT_PAYMENT'] = ins['AMT_PAYMENT'].apply(lambda x: x if x >= 0 else 0)\n",
    "    ins['PAYMENT_EFFECTUE_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_EFFECTUE_PERC'] = ins['PAYMENT_EFFECTUE_PERC'].apply(lambda x: 0 if (x == np.nan) or (x == np.inf) else x)\n",
    "    ins['PAYMENT_RESTANT'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # On différencie les : jours en retard de paiement (DPD) et jours avant paiement (DBD)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_EFFECTUE_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_RESTANT': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    # Elimination du dataframe pour libérer de la mémoire\n",
    "    del ins\n",
    "    \n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess le fichier credit_card_balance.csv\n",
    "def credit_card_balance(file = \"fichiers/credit_card_balance.csv\", nan_as_category = True):\n",
    "    cc = pd.read_csv(file, nrows = DEBUG)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    # Elimination du dataframe pour libérer de la mémoire\n",
    "    del cc\n",
    "    \n",
    "    return cc_agg  \n",
    "\n",
    "def preparation_files(bool_drift = False):\n",
    "    ##\n",
    "    #    Fonction appelant les autres fonctions de préparation d'un fichier mergé\n",
    "    #    On appel la même fonction pour créer le fichier permettant le test de data drift\n",
    "    ##\n",
    "    \n",
    "    if bool_drift == False:\n",
    "        df = application_train_test(file=\"fichiers/application_train.csv\")\n",
    "    elif bool_drift == True:\n",
    "        print(\"Préparation des fichiers pour une analyse du data_drift\")\n",
    "        df = application_train_test(file=\"fichiers/application_test.csv\")\n",
    "        \n",
    "    print(\"Préparation des fichiers : bureau and bureau_balance\")\n",
    "    bureau = bureau_and_balance()\n",
    "    print(\"Taille du fichier Bureau :\", bureau.shape)\n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "    del bureau\n",
    "\n",
    "    print(\"Préparation du fichier : previous_applications\")\n",
    "    prev = previous_applications()\n",
    "    print(\"Taille du fichier Previous applications :\", prev.shape)\n",
    "    df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "    del prev\n",
    "\n",
    "    print(\"Préparation du fichier : POS-CASH balance\")\n",
    "    pos = pos_cash()\n",
    "    print(\"Taille du fichier Pos-cash balance :\", pos.shape)\n",
    "    df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "    del pos\n",
    "\n",
    "    print(\"Préparation du fichier : installments payments\")\n",
    "    ins = installments_payments()\n",
    "    print(\"Taille du fichier Installments payments :\", ins.shape)\n",
    "    df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "    del ins\n",
    "\n",
    "    print(\"Préparation du fichier : credit card balance\")\n",
    "    cc = credit_card_balance()\n",
    "    print(\"Taille du fichier Credit card balance :\", cc.shape)\n",
    "    df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "    del cc\n",
    "\n",
    "    print(\"Création du fichier mergé...\")\n",
    "    # On créé le fichier avec les données fusionnées\n",
    "    if bool_drift == False :\n",
    "        creation_fichier_csv(df, \"fichiers/concatenate_files.csv\")\n",
    "    elif bool_drift == True :\n",
    "        creation_fichier_csv(df, \"fichiers/concatenate_files_data_drift.csv\")\n",
    "    \n",
    "    print(\"Fichier généré !\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de54ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée un transformer personnalisé sur la sélection des colonnes\n",
    "# On rajoute un élément permettant d'ignorer les colonnes manquantes\n",
    "class SelectColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, ignore_missing=False):\n",
    "        self.columns = columns\n",
    "        self.ignore_missing = ignore_missing\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.ignore_missing:\n",
    "            selected_columns = [col for col in self.columns if col in X.columns]\n",
    "        else:\n",
    "            selected_columns = self.columns\n",
    "\n",
    "        return X[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a48e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_confusion_matrix_metrics(confusion_m):\n",
    "    ##\n",
    "    # Fonction extraction les valeurs de la matrice de confusion\n",
    "    # Retourne sous la forme d'un dictionnaire\n",
    "    ##\n",
    "    vn = confusion_m[0, 0]  # vrais négatifs\n",
    "    vp = confusion_m[1, 1]  # vrais positifs\n",
    "    fp = confusion_m[0, 1]  # faux positifs\n",
    "    fn = confusion_m[1, 0]  # faux negatifs\n",
    "    \n",
    "    valeurs_matrice = {\n",
    "        \"VN\": vn,\n",
    "        \"VP\": vp,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    }\n",
    "    \n",
    "    return valeurs_matrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc96264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction affichant la matrice de confusion\n",
    "def matrice_confusion(real_labels, predicted_labels):\n",
    "\n",
    "    conf_mat = confusion_matrix(real_labels, predicted_labels, labels = [0, 1])\n",
    "\n",
    "    df_cm = pd.DataFrame(conf_mat)\n",
    "    \n",
    "    plt.figure(figsize = (6,4))\n",
    "    \n",
    "    # On garde la valeur précédente pour la restaurer ensuite\n",
    "    original_font_weight = plt.rcParams['font.weight']\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    \n",
    "    sns.heatmap(df_cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    \n",
    "    classes = [\"Bon client\", \"Mauvais client\"]\n",
    "    \n",
    "    tick_positions = np.arange(0.5, len(classes))\n",
    "    plt.xticks(tick_positions, classes)\n",
    "    plt.yticks(tick_positions, classes)\n",
    "    \n",
    "    plt.ylabel('Vrais labels', fontsize=20)\n",
    "    \n",
    "    plt.xlabel('Labels prédits', fontsize=20)\n",
    "     \n",
    "    # On ajoute du texte à la figure\n",
    "    texte_add = [\"Vrais négatifs\", \"Faux positifs\", \"Faux négatifs\", \"Vrais positifs\"]\n",
    "    n = 0\n",
    "    \n",
    "    # Pour alléger le code\n",
    "    conditions_affichage = {'ha': 'center', 'va': 'center', 'color': 'black', 'weight': 'bold'}\n",
    "    \n",
    "    for i in range(len(df_cm)):\n",
    "        for j in range(len(df_cm)):\n",
    "            # On mesure le pourcentage sur le total de chaque label\n",
    "            #pourcentage = round(df_cm.iloc[i, j] / len(real_labels) * 100, 1) \n",
    "            pourcentage = round(df_cm.iloc[i, j] / (df_cm.iloc[i, 0] + df_cm.iloc[i, 1]) * 100, 1)\n",
    "                \n",
    "            # On change la couleur du texte selon le background\n",
    "            if df_cm.iloc[i, j] > df_cm.values.max() / 2:\n",
    "                conditions_affichage['color'] = 'white'\n",
    "            else:\n",
    "                conditions_affichage['color'] = 'black'\n",
    "            \n",
    "            # On ajoute du texte\n",
    "            plt.text(j + 0.5, i + 0.625, \"(\" + str(pourcentage) + \" %)\", **conditions_affichage)\n",
    "            plt.text(j + 0.5, i + 0.75, texte_add[n], **conditions_affichage)\n",
    "            n += 1\n",
    "    \n",
    "    plt.rcParams['font.weight'] = original_font_weight\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Fonction affichant plusieurs métriques comparant les résultats de prédictions vs réels et matrice de confusion\n",
    "def results_prediction(real_labels, predicted_labels):\n",
    "\n",
    "    print(classification_report(real_labels, predicted_labels))\n",
    "\n",
    "    # on appel la fonction matrice_confusion\n",
    "    matrice_confusion(real_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620a2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affichage_score(best_params, dict_scores):\n",
    "    ##\n",
    "    #    Fonction d'affichage des scores des métriques\n",
    "    ##\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"_\" * 100)\n",
    "    print(\"Meilleurs paramètres :\", best_params)\n",
    "    print(\"Temps de recherche des meilleurs paramètres :\", dict_scores[\"training_time\"])\n",
    "    print(\"Temps d'exécution :\", dict_scores[\"elapsed_time\"])\n",
    "    print(\"_\" * 20)\n",
    "    print(\"Résultats sur échantillons d'entraînement :\")\n",
    "    print(\"Score performance : %.2f %%\" %dict_scores[\"score_performance_train\"])\n",
    "    print(\"Erreur : %.2f %%\" %dict_scores[\"errors_train\"])\n",
    "    print(\"ROC_AUC :\", dict_scores[\"roc_auc_train\"])\n",
    "    print(\"Fbeta-Score : %.2f \" %dict_scores[\"f_beta_train\"])\n",
    "    print(\"_\" * 20)\n",
    "    print(\"Résultats sur échantillons de test :\")\n",
    "    print(\"Score performance : %.2f %%\" %dict_scores[\"score_performance_test\"])\n",
    "    print(\"Erreur : %.2f %%\" %dict_scores[\"errors_test\"])\n",
    "    print(\"ROC_AUC :\", dict_scores[\"roc_auc_test\"])\n",
    "    print(\"Precision moyenne : %.2f \" %dict_scores[\"precision_mean\"])\n",
    "    print(\"Recall moyen : %.2f \" %dict_scores[\"recall_mean\"])\n",
    "    print(\"F1-Score moyen : %.2f \" %dict_scores[\"f1_mean\"])\n",
    "    print(\"Fbeta-Score : %.2f \" %dict_scores[\"f_beta_test\"])\n",
    "    print(\"_\" * 100)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4f9dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_scores(y_true_labels, y_pred_labels,):\n",
    "    ##\n",
    "    #    On calcul les valeurs moyennes de précision, recall et score F1\n",
    "    ##\n",
    "    precision_mean = round(precision_score(y_true_labels, y_pred_labels, average=\"macro\"), 2)\n",
    "    recall_mean = round(recall_score(y_true_labels, y_pred_labels, average=\"macro\"), 2)\n",
    "    f1_mean = round(f1_score(y_true_labels, y_pred_labels, average=\"macro\"), 2)\n",
    "    \n",
    "    return precision_mean, recall_mean, f1_mean\n",
    "\n",
    "def top_shape_features(shap_values, x_train_ech, feature_names):\n",
    "    ##\n",
    "    #    Fonction permettant de récupèrer les 20 features les plus importantes\n",
    "    ##\n",
    "    \n",
    "    # On calcul la somme absolue des valeurs Shap de chaque feature\n",
    "    shap_values_sum = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # On trie les sommes des valeurs shap du plus haut au plus petit\n",
    "    sorted_features_index = np.argsort(shap_values_sum)[::-1]\n",
    "\n",
    "    max_len = 20 if len(feature_names) > 20 else len(feature_names)\n",
    "    \n",
    "    # On sélectionne les N features avec valeurs les plus hautes\n",
    "    top_features_index = sorted_features_index[:max_len]  \n",
    "    \n",
    "    # On récupère les noms des features\n",
    "    top_feature_names = [feature_names[i] for i in top_features_index]\n",
    "\n",
    "    # On filtre pour n'avoir que les N features sélectionnées\n",
    "    shap_values_top = shap_values[:, top_features_index]\n",
    "    x_train_ech_top = x_train_ech[:, top_features_index]\n",
    "    \n",
    "    return x_train_ech_top, shap_values_top, top_feature_names, shap_values_sum\n",
    "\n",
    "def coefficients_importance_by_shape(model, model_name, x_train, y_train, feature_names, model_type):\n",
    "    ##\n",
    "    #    Fonction permettant un affichage indépendant avec SHAP de l'importance des variables dans les modèles\n",
    "    #    De façon local et global\n",
    "    ##\n",
    "\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)  \n",
    "        \n",
    "    # On sélectionnne un sous échantillonnage pour limiter les temps de calcul\n",
    "    ech_indices = np.random.choice(x_train.shape[0], size = 500, replace=False)\n",
    "    x_train_ech = x_train[ech_indices]\n",
    "    \n",
    "    if model_type == 0: \n",
    "        explainer = shap.LinearExplainer(model, x_train_ech)\n",
    "\n",
    "    elif model_type == 1:\n",
    "        explainer = shap.TreeExplainer(model, x_train_ech)\n",
    "\n",
    "    elif model_type == 2:\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, x_train_ech)\n",
    "        \n",
    "    elif model_type == 3:\n",
    "        explainer = shap.PermutationExplainer(model, x_train_ech)\n",
    "        \n",
    "    # check_additivity=False pour les TreeExplainer\n",
    "    shap_values = explainer.shap_values(x_train_ech) if model_type != 1 else explainer.shap_values(x_train_ech, check_additivity=False)\n",
    "    if model_type != 3:\n",
    "        expected_values = explainer.expected_value\n",
    "    \n",
    "    if len(shap_values) == 2:\n",
    "        shap_values = shap_values[1] \n",
    "        expected_values = expected_values[0]\n",
    "        \n",
    "    print(\"Utilisation de SHAP pour une explication indépendante des modèles\")\n",
    "    print(\"En quoi les variables contribue à un défaut de paiement :\")\n",
    "    \n",
    "    # On récupère les meilleures features\n",
    "    x_train_ech_top, shap_values_top, top_feature_names, feature_importances = top_shape_features(shap_values, x_train_ech, feature_names)\n",
    "    \n",
    "    # On affiche l'importance des features selon SHAP\n",
    "    plot_feature_importance(feature_importances, feature_names, model_name)\n",
    "    \n",
    "    shap.summary_plot(shap_values_top, \n",
    "                      x_train_ech_top,\n",
    "                      feature_names=[name[:20] for name in top_feature_names], \n",
    "                      max_display=20, show=False, plot_size=(10, 6))\n",
    "    \n",
    "    # On récupère les informations des objets figures\n",
    "    fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlabel(\"Valeurs de SHAP (impacts sur les sorties du modèle)\", fontsize=16)\n",
    "    ax.set_title(\"Importance des Features - Prédiction sur l'obtention d'un crédit bancaire\", fontsize=20)\n",
    "    \n",
    "    #On met une limite, pour avoir un meilleur affichage\n",
    "    #ax.set_xlim(-2, 2)\n",
    "    \n",
    "    # On récupère la barre de couleur\n",
    "    cb_ax = fig.axes[1] \n",
    "\n",
    "    # On modifie les paramètres de la barre de couleur\n",
    "    cb_ax.tick_params(labelsize=15)\n",
    "    cb_ax.set_ylabel(\"Feature value\", fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # On recherche les indices de chaque catégories\n",
    "    ech_indices_cat0 = np.where((y_train.iloc[ech_indices] == 0))[0]\n",
    "    ech_indices_cat1 = np.where((y_train.iloc[ech_indices] == 1))[0]\n",
    "\n",
    "    # On choisit aléatoirement 3 de chaque catégories\n",
    "    random_indices_cat0 = random.sample(list(ech_indices_cat0), 3)\n",
    "    random_indices_cat1 = random.sample(list(ech_indices_cat1), 3)\n",
    "    \n",
    "    if model_type != 3:\n",
    "        for i in range(2):\n",
    "            category_indices = random_indices_cat0 if i == 0 else random_indices_cat1\n",
    "        \n",
    "        for j in range(3):\n",
    "            shap.force_plot(expected_values, shap_values_top[category_indices[j]], top_feature_names, matplotlib=True)\n",
    "            plt.show()\n",
    "                  \n",
    "    #shap.summary_plot(explainer.shap_values(x_train_ech), x_train_ech, plot_type=\"bar\")\n",
    "\n",
    "def affichage_features(model, model_name, selected_cols, x_train_preprocessed, y_train, model_type = None):\n",
    "\n",
    "    # On affiche les features les plus importantes pour ce type de modèle\n",
    "    if model_type == \"DecisionTree\":\n",
    "        # On récupère le niveau d'importance des features des modèles de type arbre de décision \n",
    "        features_importances = model.feature_importances_\n",
    "\n",
    "        plot_feature_importance(features_importances, selected_cols, model_name)\n",
    "\n",
    "        coefficients_importance_by_shape(model, model_name, x_train_preprocessed, y_train, selected_cols, model_type = 1)\n",
    "\n",
    "     # On affiche les features les plus importantes pour ce type de modèle\n",
    "    elif model_type == \"Regression\":\n",
    "        # On récupère le niveau d'importance des features des modèles de type régression\n",
    "        features_importances = model.coef_[0]\n",
    "\n",
    "        plot_coefficients_importance(features_importances, selected_cols, model_name)\n",
    "\n",
    "        coefficients_importance_by_shape(model, model_name, x_train_preprocessed, y_train, selected_cols, model_type = 0)\n",
    "\n",
    "    elif model_type == \"kernel\":\n",
    "\n",
    "        coefficients_importance_by_shape(model, model_name, x_train_preprocessed, y_train, selected_cols, model_type = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a8894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(a, th):\n",
    "    if a - int(a) >= th:\n",
    "        return int(a) + 1\n",
    "    return int(a)\n",
    "\n",
    "def calcul_scores(best_estimator, x_train, y_train, x_test, y_test, beta, threshold = 0.5, \n",
    "                  my_func = None, proba = False):\n",
    "    ##\n",
    "    #    Fonction permettant de mesurer un certains nombre de métriques\n",
    "    #    Par défaut, nous utilisons un threshold de 0.5\n",
    "    #    Retourne les résultats sous la forme d'un dictionnaire\n",
    "    ##\n",
    "    \n",
    "    if proba == False :\n",
    "        # On prédit les probabilités \n",
    "        y_pred_prob_train = best_estimator.predict_proba(x_train)[:, 1]\n",
    "        y_pred_prob_test = best_estimator.predict_proba(x_test)[:, 1]\n",
    "\n",
    "        # On prédit les labels\n",
    "        y_pred_train = (best_estimator.predict_proba(x_train)[:, 1] >= (threshold)).astype(int)\n",
    "        y_pred_test = (best_estimator.predict_proba(x_test)[:, 1] >= (threshold)).astype(int)\n",
    "\n",
    "    elif proba == True :\n",
    "    # Quand le modèle prédit directement des probabilitées\n",
    "        # On prédit les probabilités \n",
    "        \n",
    "        y_pred_prob_train = best_estimator.predict(x_train, verbose = 0)\n",
    "        y_pred_prob_test = best_estimator.predict(x_test, verbose = 0)\n",
    "        \n",
    "        # On prédit les labels\n",
    "        y_pred_train = np.where(y_pred_prob_train >= threshold, 1, 0)\n",
    "        y_pred_test = np.where(y_pred_prob_test >= threshold, 1, 0)\n",
    "        \n",
    "    # On calcul le score de performance\n",
    "    if my_func != None :\n",
    "        score_performance_train = round(my_func(y_train, y_pred_train, threshold), 2)\n",
    "        score_performance_test = round(my_func(y_test, y_pred_test, threshold), 2)\n",
    "    else : \n",
    "        score_performance_train = np.nan\n",
    "        score_performance_test = np.nan\n",
    "        \n",
    "    # On calcul le score roc auc\n",
    "    roc_auc_train = round(roc_auc_score(y_train, y_pred_prob_train), 2)\n",
    "    roc_auc_test = round(roc_auc_score(y_test, y_pred_prob_test), 2)\n",
    "\n",
    "    # On calcul le pourcentage d'erreur\n",
    "    errors_train = round(100 * (1 - accuracy_score(y_train, y_pred_train)), 2)\n",
    "    errors_test = round(100 * (1 - accuracy_score(y_test, y_pred_test)), 2)\n",
    "    \n",
    "    # On calcul le f beta score\n",
    "    f_beta_train = round(fbeta_score(y_train, y_pred_train, beta=beta), 2)\n",
    "    f_beta_test = round(fbeta_score(y_test, y_pred_test, beta=beta), 2)\n",
    "\n",
    "    # On calcul les différents scores de la matrice de confusion\n",
    "    precision_mean, recall_mean, f1_mean = classification_scores(ytest, y_pred_test)\n",
    "\n",
    "    scores = {\n",
    "        \"score_performance_train\": score_performance_train,\n",
    "        \"score_performance_test\": score_performance_test,\n",
    "        \"roc_auc_train\": roc_auc_train,\n",
    "        \"roc_auc_test\": roc_auc_test,\n",
    "        \"errors_train\": errors_train,\n",
    "        \"errors_test\": errors_test,\n",
    "        \"f_beta_train\": f_beta_train,\n",
    "        \"f_beta_test\": f_beta_test,\n",
    "        \"precision_mean\": precision_mean,\n",
    "        \"recall_mean\": recall_mean,\n",
    "        \"f1_mean\": f1_mean\n",
    "    }\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1e1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_nombre_hyperparametres(param_grid):\n",
    "    ##\n",
    "    #    Fonction retournant le nombre total d'hyperparamètres testés\n",
    "    ##\n",
    "    \n",
    "    # On initialise\n",
    "    total = 1\n",
    "    \n",
    "    # On parcours le dictionnaire\n",
    "    for values in param_grid.values():\n",
    "        total *= len(values)\n",
    "    \n",
    "    # On retourne le nombre total\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d52d6",
   "metadata": {},
   "source": [
    "***\n",
    "# <a name=\"T2\">Développement du modèle : LogisticRegression</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13e664",
   "metadata": {},
   "source": [
    "### <a name = \"T2C1\">a. Score métier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd2efe",
   "metadata": {},
   "source": [
    "Création d'un score métier : \n",
    "\n",
    "- Objectif : Limiter les pertes d'argents\n",
    "\n",
    "On veut donc maximiser les bonnes prédictions et minimiser les erreurs.\n",
    "\n",
    "Certaines erreurs sont plus importantes que d'autres. \n",
    "\n",
    "Un faux négatif (donné un prêt à une personne non solvable) est plus dommeagable que ne peut pas donner un prêt.\n",
    "\n",
    "On donne donc un coût 10 fois plus important aux FN qu'aux FP.\n",
    "    \n",
    "<center>\n",
    "$\\Large \n",
    "\\text {(}\n",
    "\\frac{\\text {VN + VP}} {\\text{TOTAL}} \n",
    "- \n",
    "\\text {(}\n",
    "\\frac{\n",
    "\\frac{\\text {10 * FN}} {\\text {TOTAL_P}} \n",
    "+ \n",
    "\\frac{\\text {FP}} {\\text {TOTAL_N}} \n",
    "}\n",
    "{11}\n",
    "\\text {)}\n",
    "\\text {)}\n",
    "* 100\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3493983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_scoring(y_true_labels, y_pred_labels, th = 0.5):\n",
    "    \n",
    "    # On prédit les labels avec les probabilitées, dépendant du threshold utilisé\n",
    "    y_pred_labels_th = (y_pred_labels >= th).astype(int)\n",
    "    \n",
    "    confusion_m = confusion_matrix(y_true_labels, y_pred_labels_th)\n",
    "    \n",
    "    valeurs_matrice_dict = extract_confusion_matrix_metrics(confusion_m)\n",
    "    \n",
    "    Total_NP = sum(v for v in valeurs_matrice_dict.values())\n",
    "\n",
    "    Total_N = valeurs_matrice_dict['FP'] + valeurs_matrice_dict['VN']\n",
    "    Total_P = valeurs_matrice_dict['VP'] + valeurs_matrice_dict['FN']\n",
    "    \n",
    "    Total_NP = Total_N + Total_P\n",
    "    \n",
    "    FN = valeurs_matrice_dict['FN'] \n",
    "\n",
    "    FP = valeurs_matrice_dict['FP'] \n",
    "    \n",
    "    VN = valeurs_matrice_dict['VN']\n",
    "    \n",
    "    VP = valeurs_matrice_dict['VP']\n",
    "    \n",
    "    # En divisant par 11, on a un score entre 0 et 100\n",
    "    return (((VN + VP) / (Total_NP)) - (((10*FN / Total_P) + (FP / Total_N)) / 11)) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b563318",
   "metadata": {},
   "source": [
    "On va tester la fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db8331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sépare les données\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(data.drop(\"TARGET\", axis = 1), data.TARGET, test_size = 0.15, stratify = data[\"TARGET\"], random_state = 42)\n",
    "\n",
    "dataset_name = \"optimisation avec l'ensemble des variables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04c90c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille et dimension des données d'entraînement : (261380, 796)\n",
      "Taille et dimension des données de test : (46127, 796)\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille et dimension des données d'entraînement :\", xtrain.shape)\n",
    "print(\"Taille et dimension des données de test :\", xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea30d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les colonnes d'intérêts\n",
    "\n",
    "with open('selected_col_ANOVA.pkl', 'rb') as selected_col_ANOVA_file:\n",
    "    selected_col_ANOVA = pickle.load(selected_col_ANOVA_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffbcf5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_col_ANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc32fc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids pour la classe 0 : 0.50\n",
      "Poids pour la classe 1: 6.20\n"
     ]
    }
   ],
   "source": [
    "# On compte le nombre dans chaque classe\n",
    "neg, pos = np.bincount(ytrain)\n",
    "total = neg + pos\n",
    "\n",
    "poids_classe_0 = round((1 / neg) * (total / 2.0), 1)\n",
    "poids_classe_1 = round((1 / pos) * (total / 2.0), 1)\n",
    "\n",
    "poids_classes = {0: poids_classe_0, 1: poids_classe_1}\n",
    "\n",
    "print(\"Poids pour la classe 0 : {:.2f}\".format(poids_classe_0))\n",
    "print(\"Poids pour la classe 1: {:.2f}\".format(poids_classe_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV 1/3] END feature_selection__k=100, scaler=StandardScaler(); custom_score: (test=37.466) roc_auc: (test=0.755) total time=   9.3s\n"
     ]
    }
   ],
   "source": [
    "# On instancie le modèle avec les meilleurs paramètres trouvés (différence sur le class_weight)\n",
    "modele_LR = LogisticRegression(class_weight={0 : .5, 1 : 6.2},\n",
    "                               C=0.1,\n",
    "                               max_iter=100, \n",
    "                               n_jobs = -1, \n",
    "                               random_state = 42)\n",
    "\n",
    "# On test quelques scaler et un nombre de k feature\n",
    "param_grid_search = {\n",
    "    \"scaler\" : [StandardScaler(), RobustScaler(), MinMaxScaler()],\n",
    "    \"feature_selection__k\" : [100, 200]\n",
    "}\n",
    "\n",
    "# Permet de sélectionner les colonnes d'intérêts (passer de 700 à 200)\n",
    "column_selector = SelectColumns(columns=selected_col_ANOVA, ignore_missing=True)\n",
    "    \n",
    "pipe = Pipeline([\n",
    "        ('select_columns', column_selector),\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif)),\n",
    "        ('scaler', None),\n",
    "        ('model', modele_LR)\n",
    "    ])\n",
    "\n",
    "# On utilise notre scorer, on veut maximiser le score\n",
    "# Besoin d'avoir les probabilité\n",
    "my_scorer = make_scorer(my_custom_scoring, needs_proba = True, greater_is_better=True)\n",
    "    \n",
    "# On veut suivre notre scorer et roc_auc\n",
    "scorers = {\n",
    "    \"custom_score\" : my_scorer,\n",
    "    \"roc_auc\" : \"roc_auc\"\n",
    "    }\n",
    "grid = GridSearchCV(pipe, param_grid_search, cv = 3, scoring = scorers, refit = \"custom_score\", verbose = 3)\n",
    "\n",
    "# On fit sur les données d'entrainement\n",
    "grid.fit(xtrain, ytrain)\n",
    "\n",
    "# On récupère les meilleurs paramètres \n",
    "best_param_pipe = grid.best_estimator_\n",
    "\n",
    "# On test sur les données d'entraînements\n",
    "results_prediction(ytest, best_param_pipe.predict(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd9a8b",
   "metadata": {},
   "source": [
    "On obtient de très bon résultats avec le scorer développé. \n",
    "\n",
    "On peut tenter d'améliorer notre modèle en optimisant le seuil utiliser pour la prédiction (la probabilité) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d2e48",
   "metadata": {},
   "source": [
    "### <a name = \"T2C2\">b. Recherche du meilleur seuil</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf61ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_seuil(best_pipe, x, y):\n",
    "    ##\n",
    "    #    Fonction permettant de rechercher le meilleur score (selon notre scorer) par rapport au seuil\n",
    "    #    Premier balayage assez large, puis recherche sur une fenêtre plus petite\n",
    "    ##\n",
    "    \n",
    "    # Pour conserver les résultats\n",
    "    scores_seuil_1 = []\n",
    "    scores_seuil_2 = []\n",
    "\n",
    "    # Itération nestée, pour aller plus vite dans la recherche du meilleur seuil\n",
    "    for i, scores in enumerate([scores_seuil_1, scores_seuil_2]):\n",
    "        \n",
    "        # Première itération, on recherche sur une gamme très haute\n",
    "        if i == 0:\n",
    "            fenetre = np.arange(0, 1.01, 0.1)\n",
    "            \n",
    "        # Deuxième itération, on recherche sur une gamme plus précise, autour de la meilleur valeur trouvée précédemment\n",
    "        else :\n",
    "            fenetre = np.arange(meilleur_seuil - 0.1, meilleur_seuil + 0.1, 0.01)\n",
    "\n",
    "        # On va tester un certain nombre de valeur de seuil/threshold\n",
    "        for seuil in fenetre:\n",
    "            # Prédiction selon le seuil\n",
    "            predict_pro = (best_pipe.predict_proba(x)[:, 1] >= (seuil)).astype(int)\n",
    "            \n",
    "            # On mesure le score métier\n",
    "            score_metier_seuil = round(my_custom_scoring(y, predict_pro, seuil), 2)\n",
    "            \n",
    "            # On stocke le résultat\n",
    "            scores.append([round(seuil, 2), score_metier_seuil])\n",
    "\n",
    "        scores = pd.DataFrame(scores, columns = [\"Seuil\", \"Score\"])\n",
    "        \n",
    "        # On recherche le meilleur score et le meilleur seuil\n",
    "        meilleur_seuil = scores.loc[scores.Score == scores.Score.max(), \"Seuil\"].values[0]\n",
    "\n",
    "        meilleur_score = scores.loc[scores.Score == scores.Score.max(), \"Score\"].values[0]\n",
    "        \n",
    "    # On assemble les 2 scores\n",
    "    scores_seuil = scores_seuil_1 + scores_seuil_2\n",
    "    \n",
    "    # On transforme en dataframe et on élimine les duplicatas\n",
    "    scores_seuil = pd.DataFrame(scores_seuil, columns = [\"Seuil\", \"Score\"]).drop_duplicates()\n",
    "    \n",
    "    return scores_seuil, meilleur_seuil, meilleur_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2429261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affichage_seuil_score(df_score_seuil, b_seuil, b_score):\n",
    "    ##\n",
    "    #    Fonction permettant d'afficher les résultats de la recherche de seuil \n",
    "    ##\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(\"Analyse du score par rapport à la valeur seuil\\n Meilleur score : {} Meilleur Seuil : {}\".format(b_score, b_seuil), \n",
    "              size = 20)\n",
    "    sns.lineplot(df_score_seuil, x = \"Seuil\", y = \"Score\", label='Courbe')\n",
    "    # Pour identifier le meilleur\n",
    "    plt.scatter(b_seuil, b_score, color='red', label='Meilleur')\n",
    "    \n",
    "    # Pour identifier la base\n",
    "    basique_score = float(df_score_seuil.loc[df_score_seuil.Seuil == 0.5, \"Score\"])\n",
    "    \n",
    "    plt.scatter(0.5, basique_score, color='green', label='Base')\n",
    "    # Pour identifier l'ensemble des scores > à la valeur de base\n",
    "    plt.axhline(y=basique_score, color='green', linestyle='--', label = \"Basique ligne\")\n",
    "\n",
    "    plt.xlim(0, 1)\n",
    "    plt.xlabel(\"Seuils\", size = 18)\n",
    "    plt.ylabel(\"Scores\", size = 18)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_seuil, meilleur_seuil, meilleur_score = search_best_seuil(best_param_pipe, xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "affichage_seuil_score(scores_seuil, meilleur_seuil, meilleur_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a4244",
   "metadata": {},
   "source": [
    "On observe que le meilleur seuil de prédiction n'est pas à la valeur par défaut 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d750e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données d'entraînements\n",
    "\n",
    "# Nous réalisons la prédiction avec le meilleur seuil\n",
    "predict_best_seuil = (best_param_pipe.predict_proba(xtrain)[:, 1] >= (meilleur_seuil)).astype(int)\n",
    "\n",
    "# On regarde les nouveaux résultats obtenus\n",
    "results_prediction(ytrain, predict_best_seuil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be618a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données test\n",
    "\n",
    "# Nous réalisons la prédiction avec le meilleur seuil\n",
    "predict_best_seuil = (best_param_pipe.predict_proba(xtest)[:, 1] >= (meilleur_seuil)).astype(int)\n",
    "\n",
    "# On regarde les nouveaux résultats obtenus\n",
    "results_prediction(ytest, predict_best_seuil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fb58e",
   "metadata": {},
   "source": [
    "**Observations** : \n",
    "\n",
    "On observe qu'en modifiant le seuil par rapport à la valeur de départ, on diminue légèrement la détection des clients insolvables (augmentation des Faux négatifs et diminution des Vrais positifs) vers une augmentation des clients solvables (augmentation des Vrais négatifs et diminution des Faux positifs). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9428a0",
   "metadata": {},
   "source": [
    "A en discuter avec les membres de l'équipe, sur la direction à prendre sur le scorer. On pourrait le modifier pour qu'il tende plus vers la détection des vrais positifs en contrepartie d'une augmentation des faux positifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ae176",
   "metadata": {},
   "source": [
    "### <a name = \"T2C3\">c. Recherche des meilleurs hyperparamètres</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42318012",
   "metadata": {},
   "source": [
    "Maintenant que nous avons développer un score métier pour la recherche des hyperparamètres et d'une valeur seuil, nous allons finaliser le modèle :\n",
    "- Tester un plus grand nombre d'hyperparamètres.\n",
    "- Identifier la meilleur valeur seuil pour le score métier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainer_feature_save_model(df, selected_cols, model):\n",
    "    ##\n",
    "    #    Fonction permettant de sauvegarder un objet explainer et l'importance des features dans un fichier pickle et csv\n",
    "    ##\n",
    "    \n",
    "    # on génére l'objet explainer\n",
    "    explainer = shap.LinearExplainer(model, df)\n",
    "    \n",
    "    with open('explainer_model.pkl', 'wb') as explainer_file:\n",
    "        pickle.dump(explainer, explainer_file)\n",
    "    \n",
    "    # On récupère les valeurs\n",
    "    shap_values = explainer(df)\n",
    "    \n",
    "    # On mesure l'importance de chaque feature\n",
    "    average_absolute_shap_values = np.round(np.abs(shap_values_model.values).mean(axis=0), 2)\n",
    "\n",
    "    # on crée un dataframe\n",
    "    shap_values_df = pd.DataFrame({'Features': selected_cols, 'Importance': average_absolute_shap_values})\n",
    "\n",
    "    # on classe de la feature la plus importante à la plus faible\n",
    "    shap_values_df = shap_values_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # On crée le fichier csv\n",
    "    shap_values_df.to_csv(\"shap_values_model.csv\", index = False, sep = \",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eefaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelle version de la fonction search_model_params avec recherche du meilleur seuil \n",
    "def search_model_params_score_metier(model_name, model, params, x_train, y_train, x_test, y_test, cols_model, model_type = None):\n",
    "    \n",
    "    # Permet de sélectionner les colonnes d'intérêts\n",
    "    column_selector = SelectColumns(columns=cols_model, ignore_missing=True)\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "            ('select_columns', column_selector),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('feature_elimination', VarianceThreshold(threshold=0.0)),\n",
    "            ('feature_selection', SelectKBest(score_func=f_classif)),\n",
    "            ('scaler', None),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "    # On utilise notre scorer, on veut maximiser le score\n",
    "    # Besoin d'avoir les probabilité\n",
    "    my_scorer = make_scorer(my_custom_scoring, needs_proba = True, greater_is_better=True)    \n",
    "\n",
    "    # On crée un dictionnaire qui contiendra les différents scores\n",
    "    scores = {}\n",
    "\n",
    "    # On utilise notre scorer, on veut maximiser le score\n",
    "    my_scorer = make_scorer(my_custom_scoring, needs_proba = True, greater_is_better=True)\n",
    "    precision_scorer = make_scorer(precision_score, zero_division=1, greater_is_better=True)\n",
    "    recall_scorer = make_scorer(recall_score, zero_division=1, greater_is_better=True)\n",
    "    f1_scorer = make_scorer(f1_score, zero_division=1, greater_is_better=True)\n",
    "    beta = 2\n",
    "    f_beta_scorer = make_scorer(fbeta_score, zero_division=1, beta=beta, greater_is_better=True)\n",
    "\n",
    "    # On veut suivre plusieurs scores\n",
    "    scorers = {\n",
    "    \"custom_score\" : my_scorer,\n",
    "    \"roc_auc\" : \"roc_auc\",\n",
    "    \"precision_scorer\" : precision_scorer,\n",
    "    \"recall_scorer\" : recall_scorer,\n",
    "    \"f1_scorer\" : f1_scorer,\n",
    "    \"f_beta\" : f_beta_scorer,\n",
    "    }\n",
    "\n",
    "    time0 = time.time()\n",
    "\n",
    "    # On instancie un StratifiedKFold pour garder la répartition dans le gridsearch\n",
    "    # Je réduit à 3 pour augmenter la vitesse d'éxecution, les tests préliminaires ont montré que 3 était suffisant \n",
    "    stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    # On réalise un GridSearchCV pour trouver les meilleurs hyperparamètres pour les modèles de type non \"arbre de décision\"\n",
    "    print(\"Nous testons %d hyperparamètres\" %calcul_nombre_hyperparametres(params)) \n",
    "\n",
    "    grid = GridSearchCV(pipe, params, cv = stratified_kfold, scoring = scorers, refit = \"custom_score\", \n",
    "                        verbose = 1)\n",
    "\n",
    "\n",
    "    # On fit sur les données d'entrainement\n",
    "    grid.fit(x_train, y_train)\n",
    "\n",
    "    time1 = time.time()\n",
    "\n",
    "    # On mesure le temps d'entrainement\n",
    "    scores[\"training_time\"] = round(time1 - time0, 1)\n",
    "\n",
    "    # On récupère les meilleurs paramètres \n",
    "    best_param_pipe = grid.best_estimator_\n",
    "\n",
    "    # On réalise une cross évaluation\n",
    "    cross_scores = cross_val_score(best_param_pipe, x_train, y_train, cv=stratified_kfold, scoring=my_scorer)\n",
    "    print(\"Cross-validation score : %0.2f avec un écart-type de %0.2f \\n\" \n",
    "              % (cross_scores.mean(), cross_scores.std()))\n",
    "\n",
    "    # On mesure les différentes métriques et on ajoute au dictionnaire\n",
    "    scores.update(calcul_scores(best_param_pipe, x_train, y_train, x_test, y_test, beta, my_func = my_custom_scoring))\n",
    "\n",
    "    # On mesure le temps d'application du meilleur modèle\n",
    "    scores[\"elapsed_time\"] = round(time.time() - time1, 1)\n",
    "\n",
    "    # On affiche les différents résultats des métriques\n",
    "    affichage_score(grid.best_params_, scores)\n",
    "\n",
    "    # On affiche la matrice de confusion\n",
    "    results_prediction(y_test, best_param_pipe.predict(x_test))\n",
    "\n",
    "    # Recherche du meilleur seuil\n",
    "    scores_seuil, meilleur_seuil, meilleur_score = search_best_seuil(best_param_pipe, x_train, y_train)\n",
    "\n",
    "    # Affichage de la recherche\n",
    "    affichage_seuil_score(scores_seuil, meilleur_seuil, meilleur_score)\n",
    "\n",
    "    new_scores = {\"training_time\" : scores[\"training_time\"],\n",
    "                 \"elapsed_time\" : scores[\"elapsed_time\"]}\n",
    "    \n",
    "    # On mesure les nouveaux scores obtenus avec le meilleur seuil\n",
    "    new_scores.update(calcul_scores(best_param_pipe, \n",
    "                                    x_train, \n",
    "                                    y_train,\n",
    "                                    x_test, \n",
    "                                    y_test, \n",
    "                                    beta, \n",
    "                                    meilleur_seuil, \n",
    "                                    my_func = my_custom_scoring))\n",
    "\n",
    "    print(\"Affichages nouveaux scores avec le meilleur seuil\")\n",
    "\n",
    "    # On affiche les différents résultats des métriques\n",
    "    affichage_score(grid.best_params_, new_scores)\n",
    "    \n",
    "    # on ajoute le score de seuil au modèle\n",
    "    new_scores[\"Valeur seuil\"] = meilleur_seuil\n",
    "    \n",
    "    # Nous réalisons la prédiction avec le meilleur seuil\n",
    "    predict_best_seuil = (best_param_pipe.predict_proba(x_test)[:, 1] >= (meilleur_seuil)).astype(int)\n",
    "\n",
    "    # On regarde les nouveaux résultats obtenus\n",
    "    results_prediction(y_test, predict_best_seuil)\n",
    "\n",
    "    # On transform x_train selon le pipeline (en enlevant les 2 dernières étapes)\n",
    "    x_train_preprocessed = best_param_pipe[:-1].transform(x_train)\n",
    "\n",
    "    # On récupère le nom des colonnes sélectionnées\n",
    "    selected_cols = xtrain.loc[:, ~xtrain.columns.isin(cols_model)].columns[grid.best_estimator_['feature_selection'].get_support()]\n",
    "\n",
    "    affichage_features(best_param_pipe.named_steps['model'], modele_name, selected_cols, x_train_preprocessed, y_train)\n",
    "    \n",
    "    # On crée et on sauvegarde un objet explainer et l'importance des features\n",
    "    # Si on doit expliquer le modèle sur le dashboard\n",
    "    x_train_preprocessed = pd.DataFrame(x_train_preprocessed, columns = selected_cols)\n",
    "    explainer_feature_save_model(x_train_preprocessed, selected_cols, best_param_pipe.named_steps['model'])\n",
    "        \n",
    "    # On retourne les différentes métriques et le meilleur modèle\n",
    "    return new_scores, best_param_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"FINAL_VERSION_Developpement_dun_modele_de_bank_scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343667ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On instancie le modèle\n",
    "modele_LR = LogisticRegression(n_jobs = -1, random_state = 42)\n",
    "\n",
    "modele_name = \"LogisticRegression\"\n",
    "\n",
    "model_type = \"Regression\"\n",
    "\n",
    "# On test des paramètres proche de ce qu'on a testé (et pré test)\n",
    "param_grid_search = {\n",
    "    \"imputer\" : [SimpleImputer(strategy='median')],\n",
    "    \"scaler\" : [StandardScaler()],\n",
    "    \"feature_selection__k\" : [150],\n",
    "    \"model__C\" : [0.1],\n",
    "    \"model__max_iter\" : [1000],\n",
    "    \"model__class_weight\" : ['balanced'], \n",
    "    \"model__solver\" : ['sag'],\n",
    "    \"model__penalty\" : ['l2'],\n",
    "    'model__tol': [1e-4]\n",
    "}\n",
    "\n",
    "# On commence l\"enregistrement mlflow \n",
    "with mlflow.start_run(run_name=modele_name) as run:\n",
    "\n",
    "    # On récupère les résultats après application de la fonction search_model_params\n",
    "    results, model = search_model_params_score_metier(modele_name, modele_LR, param_grid_search, \n",
    "                                 xtrain, ytrain, xtest, ytest, selected_col_ANOVA, model_type)\n",
    "    \n",
    "    # On catalogue le dataset utilisé, les paramètres grids, les métriques produites et le modèle retenu  \n",
    "    log_mlflow(param_grid_search, data, dataset_name, results, model)\n",
    "\n",
    "# On ajoute à la liste de résultats des modèles\n",
    "resultats_models.append([modele_name+\"_optimisé\"] + list(results.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On instancie le modèle\n",
    "modele_LR = LogisticRegression(n_jobs = -1, random_state = 42)\n",
    "\n",
    "modele_name = \"LogisticRegression\"\n",
    "\n",
    "model_type = \"Regression\"\n",
    "\n",
    "# On test des paramètres proche de ce qu'on a testé (et pré test)\n",
    "param_grid_search = {\n",
    "    \"imputer\" : [SimpleImputer(strategy='median')],\n",
    "    \"scaler\" : [StandardScaler()],\n",
    "    \"feature_selection__k\" : [150, 200],\n",
    "    \"model__C\" : [0.1, 1, 2],\n",
    "    \"model__max_iter\" : [100, 1000],\n",
    "    \"model__class_weight\" : ['balanced', {0:0.5, 1:6.2}], \n",
    "    \"model__solver\" : ['sag', 'lbfgs'],\n",
    "    \"model__penalty\" : ['l2'],\n",
    "    'model__tol': [1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "# On commence l\"enregistrement mlflow \n",
    "with mlflow.start_run(run_name=modele_name) as run:\n",
    "\n",
    "    # On récupère les résultats après application de la fonction search_model_params\n",
    "    results, model = search_model_params_score_metier(modele_name, modele_LR, param_grid_search, \n",
    "                                 xtrain, ytrain, xtest, ytest, selected_col_ANOVA, model_type)\n",
    "    \n",
    "    # On catalogue le dataset utilisé, les paramètres grids, les métriques produites et le modèle retenu  \n",
    "    log_mlflow(param_grid_search, data, dataset_name, results, model)\n",
    "\n",
    "# On ajoute à la liste de résultats des modèles\n",
    "resultats_models.append([modele_name+\"_optimisé\"] + list(results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174b4bb",
   "metadata": {},
   "source": [
    "### <a name = \"T3C4\">d. Quelques observation sur le modèle</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a646d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ed1c73f",
   "metadata": {},
   "source": [
    "### <a name = \"T3C5\">e. Enregistrement du modèle</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83790f",
   "metadata": {},
   "source": [
    "Nous avons établis que c'était le meilleur modèle, nous allons l'enregistrer pour pouvoir le charger plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère l'heure actuelle\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# On crée un nom dynamique pour la génération du modèle\n",
    "save_model_name = f\"banking_model_{timestamp}\"\n",
    "\n",
    "# On enregistre le modèle\n",
    "save_model(model, save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On test que notre modèle a bien été enregistré et qu'il est chargeable.\n",
    "test_load_model = mlflow.sklearn.load_model(save_model_name)\n",
    "\n",
    "test_load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900edbd",
   "metadata": {},
   "source": [
    "***\n",
    "# <a name=\"T4\">Dérive du modèle</a>\n",
    "### <a name = \"T4C1\">a. Evidently</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abafbd",
   "metadata": {},
   "source": [
    "Pour tester le data drift, qui correspond à un changement dans la distribution des données. Nous allons tester les données provenant du fichier application_test.csv. Ce fichier n'avait pas été utilisé, car il ne possède pas la valeur TARGET. Pour le data drift, on ne s'en préocupe pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e624b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f02baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la liste de fichiers dans le dossier\n",
    "file_list = os.listdir(path_data + \"/fichiers\")\n",
    "\n",
    "# On filtre sur les fichiers csv\n",
    "csv_files = [\"fichiers/\" + file for file in file_list if file.endswith('.csv')]\n",
    "\n",
    "# Si l'opération de merge n'a pas déjà été réalisée, on la réalise\n",
    "if \"fichiers/concatenate_files_data_drift.csv\" not in (csv_files):\n",
    "    print(\"Préparation du fichier...\")\n",
    "    data_drift = preparation_files(bool_drift = True)\n",
    "\n",
    "else:\n",
    "    print(\"Fichier déjà généré\")\n",
    "    data_drift = pd.read_csv(\"fichiers/concatenate_files_data_drift.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfa840",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drift.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2b64c",
   "metadata": {},
   "source": [
    "On peut traiter les données directement à l'aide du pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06774c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_evidently = model[0].transform(data)\n",
    "data_drift_evidently = model[0].transform(data_drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f55574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie que le traitement a été réalisé\n",
    "print(\"Données initiales :\", data_evidently.shape)\n",
    "print(\"Données de test : \", data_drift_evidently.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448f62c",
   "metadata": {},
   "source": [
    "Une variable est manquante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e496e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in data_evidently.columns if col not in data_drift_evidently.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf7703e",
   "metadata": {},
   "source": [
    "C'est une colonne issue d'un encodage, ça veut dire que dans NAME_INCOME_TYPE il n'y a pas de valeur Maternity leave dans les données test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1f329",
   "metadata": {},
   "source": [
    "### <a name = \"T3C2\">b. Résultat</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdd16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"\"\n",
    "\n",
    "html_file_name = \"rapport_data_drift.html\"\n",
    "\n",
    "# On vérifie si un fichier a déjà été généré ou non\n",
    "if os.path.isfile(os.path.join(path_data, html_file_name)):\n",
    "    # Si oui, on affiche le fichier\n",
    "    display(IFrame(os.path.join(path_data, html_file_name), width=1000, height=1000))\n",
    "else:\n",
    "    # Si non, on génére un fichier\n",
    "    print(f\"Fichier '{html_file_name}' non trouvé. \\n Génération d'un nouveau fichier...\")\n",
    "    data_drift_report = Report(metrics=[DataDriftPreset(),])\n",
    "\n",
    "    # On compare les données train contre les données test, sur les variables utilisées dans le modèle\n",
    "    data_drift_report.run(current_data=data_drift_evidently, reference_data=data_drift, column_mapping=None)\n",
    "    \n",
    "    display(data_drift_report.show(mode='inline'))\n",
    "\n",
    "    # On génére un rapport\n",
    "    data_drift_report.save_html(html_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbb970",
   "metadata": {},
   "source": [
    "Nous avons très peu de data_drift, seulement 10 variables.\n",
    "\n",
    "Seulement sur les 3-4 premières ce drift est important.\n",
    "\n",
    "Donc, le modèle identifié restera assez pertinent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac67628",
   "metadata": {},
   "source": [
    "On peut regarder maintenant, sur les données que le modèle a vraiment utilisé et après traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On identifie les colonnes manquantes et on remplace par des valeurs nulles, ça permet de traiter avec le modèle\n",
    "missing_columns = [col for col in data.columns if col not in data_drift.columns]\n",
    "\n",
    "# On ajoute à data_drift les colonnes manquantes\n",
    "for col in missing_columns:\n",
    "    data_drift[col] = pd.Series([np.nan] * len(data_drift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère le nom des features\n",
    "feature_names = data_evidently.columns[model[-3].get_support()]\n",
    "\n",
    "data_evidently_model_treated = pd.DataFrame(model[:-1].transform(data), columns = feature_names)\n",
    "data_drift_evidently_model_treated = pd.DataFrame(model[:-1].transform(data_drift), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie que le traitement a été réalisé\n",
    "print(\"Données initiales :\", data_evidently_model_treated.shape)\n",
    "print(\"Données de test : \", data_drift_evidently_model_treated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"\"\n",
    "\n",
    "html_file_name = \"rapport_data_drift_traited.html\"\n",
    "\n",
    "# On vérifie si un fichier a déjà été généré ou non\n",
    "if os.path.isfile(os.path.join(path_data, html_file_name)):\n",
    "    # Si oui, on affiche le fichier\n",
    "    display(IFrame(os.path.join(path_data, html_file_name), width=1000, height=1000))\n",
    "else:\n",
    "    # Si non, on génére un fichier\n",
    "    print(f\"Fichier '{html_file_name}' non trouvé. \\n Génération d'un nouveau fichier...\")\n",
    "    data_drift_report = Report(metrics=[DataDriftPreset(),])\n",
    "\n",
    "    # On compare les données train contre les données test, sur les variables utilisées dans le modèle\n",
    "    data_drift_report.run(current_data=data_drift_evidently_model_treated, \n",
    "                          reference_data=data_evidently_model_treated, \n",
    "                          column_mapping=None)\n",
    "    \n",
    "    display(data_drift_report.show(mode='inline'))\n",
    "\n",
    "    # On génére un rapport\n",
    "    data_drift_report.save_html(html_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "On ne retrouve un data shift que sur peu de features, ce qui est positif.\n",
    "\n",
    "Le data drift score n'est pas très important sur ces features, donc le modèle utilise des features très robustes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143cf3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
